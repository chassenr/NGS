# Analysis of 16S amplicons 

# Based on a script adpated from the OSD 16S pipeline (https://colab.mpi-bremen.de/micro-b3/svn/analysis-scripts/trunk/osd-analysis/osd-pre-processing/16S/lgc/primer-clipping/02primer-clipping.sh author: Antonio Fernandez-Guerra) by Christiane Hassenrueck at the MPI Bremen. Swarming and (some of) the classification (additional credits: Chris Quast, Josephine Rapp, Pier Buttigieg) differs from the OSD pipeline.
# reads generated on a MiSeq, 2x300bp
# customized for sequencing at CeBiTec in Bielefeld (Halina Tegetmeyer)
# input seqeunces do NOT contain barcode and adapter sequences anymore, but still contain primer sequences
# to avoid weird line endings (^M) work exclusively on linux when preparing and creating files (or make sure you convert to Linux line endings)
# workflow optimzed for parallelization on the SGE cluster of the MPI Bremen

#######################################
#
# Preliminaries...
#
#######################################

# Directories...
# all input files and custom scripts are located in the working directory

# set variables, program and script locations
#make sure all qsub scripts have +x permissions
CUTADAPT="/bioinf/software/anaconda/anaconda-2.1/bin/cutadapt"
CLIP_qsub="clipping_qsub.sh"
TRIMMOMATIC="/bioinf/software/trimmomatic/trimmomatic-0.32/trimmomatic-0.32.jar"
TRIM_qsub="trimming_qsub.sh"
PEAR="/bioinf/software/pear/pear-0.9.5/bin/pear"
PEAR_qsub="merging_qsub.sh"
FASTQC="/bioinf/software/fastqc/fastqc-0.11.2/fastqc"
FASTQC_qsub="fastqc_qsub.sh"
REFORMAT="/bioinf/software/bbmap/bbmap-34.00/reformat.sh"
REFORMAT_qsub="reformat_qsub.sh"
DEREPLICATE_qsub="dereplicate_qsub.sh"
SWARM="/home/chassenr/programs/swarm/bin/swarm"
AMPLICONTAB="amplicon_contingency_table.py"
SINA="/bioinf/software/sina/sina-1.2.10/sina"
SINA_PT="/bioinf/projects/silva/seeds/classification/SILVA_119_SSURef_Nr.arb"
SINA_qsub="submission_sina2.sh" #script located in /NGS/META_G_T/
NSAMPLE="13"
#other dependencies:
#python
#awk
#java 1.8


#######################################
#
# General steps...
#
#######################################

#step 0: shorten file names, rename to running number
#step 1: primer clipping (cutadapt, parameters from OSD workflow)
#step 2: quality trimming (trimmomatic) - optional (for long reads recommended after merging: per base quality might improve throughout merging progress)
#step 3: merging (PEAR)
#step 4: quality trimming and control (trimmomatic - optional, fastqc)
#step 5: swarm otu clustering (swarm)
#step 6: taxonomic classification of seed sequences
#step 7: further analysis (R)


######################################
#
# Analysis pipeline
#
######################################


###step 0: shorten file names, rename to running number

#moving original files to separate directory
mkdir Original
mv *.fastq ./Original/

#creating directory for renamed files
mkdir Renamed

#save original file names
ls -1v ./Original/*R1.fastq > ./Renamed/originalR1
ls -1v ./Original/*R2.fastq > ./Renamed/originalR2

#copy original files to new file names
new=1
for i in $(ls -1v ./Original/*R1.fastq)
do
  cp ${i} ./Renamed/${new}"_R1.fastq"
  ((new++))
done

new=1
for i in $(ls -1v ./Original/*R2.fastq)
do
  cp ${i} ./Renamed/${new}"_R2.fastq"
  ((new++))
done

#check that the renaming was done correctly
ls -1v ./Renamed/[0-9]*_R1.fastq > ./Renamed/renamedR1
ls -1v ./Renamed/[0-9]*_R2.fastq > ./Renamed/renamedR2

paste ./Renamed/originalR1 ./Renamed/renamedR1 > ./Renamed/fileID_R1
paste ./Renamed/originalR2 ./Renamed/renamedR2 > ./Renamed/fileID_R2

while read line ; do
  diff $(echo "$line")
done < ./Renamed/fileID_R1

while read line ; do
  diff $(echo "$line")
done < ./Renamed/fileID_R2


###step 1: primer clipping 
# (adapted from https://colab.mpi-bremen.de/micro-b3/svn/analysis-scripts/trunk/osd-analysis/osd-pre-processing/16S/lgc/primer-clipping/02primer-clipping.sh)

# for each sample the following commands will remove the primer sequence when found (or discard the read)
# it will search both R1 and R2 for both the forward and the reverse primer sequence and
# if the insert is inverted, it will turn the sequences in the right orientation to make life easier downstream...

# Input your primer sequences, use ^ to anchor to the beginning of line
FBC=^CCTACGGGNGGCWGCAG # forward primer
RBC=^GACTACHVGGGTATCTAATCC # reverse primer

OFWD=16 # length of forward primer (17) - 1
OREV=20 # length of reverse primer (21) - 1

# Set the proportion of mismatches allowed when matching the primer sequences
ERROR=0.16

#create directory for output of primer clipping
mkdir Clipped

#primer clipping on SGE cluster
qsub -t 1-${NSAMPLE} -v CUTADAPT=${CUTADAPT},FBC=${FBC},RBC=${RBC},OFWD=${OFWD},OREV=${OREV},ERROR=${ERROR} ${CLIP_qsub}

#cleaning up directories
mkdir ./Clipped/Clipped_logs
mv ${CLIP_qsub}.* ./Clipped/Clipped_logs/
mv ./Clipped/*.log ./Clipped/Clipped_logs/
mv ./Clipped/*.info ./Clipped/Clipped_logs/


###step 2: quality trimming 
# for long inserts (450ish and onwards) recommended after merging 
# (you need all the bases you can get, additionally two identical bases with low quality which are merged will generally have a higher quality score.
# shorter inserts have more overlap and can afford some loss)
# for the standard bacterial illumina insert we can do it before merging

# ptrim = identical sequence headers with R1 and R2 strim = single output, complementary reads removed
# SLIDINGWINDOW:4:10 is the absolute minimum! 4:15 recommended
# argument order matters! MINLEN should come after trimming.

# for each sample, trim the clipped reads with a sliding window of 4 and a quality threshold of 15
# Discard reads less than 100 bps.

#creating directory for output of quality trimming
mkdir Trimmed

qsub -t 1-${NSAMPLE} -v TRIMMOMATIC=${TRIMMOMATIC} ${TRIM_qsub}

#cleaning up directories
mkdir ./Trimmed/Trimmed_logs
mv ${TRIM_qsub}.* ./Trimmed/Trimmed_logs
mv ./Trimmed/*.log ./Trimmed/Trimmed_logs


###step 3: read merging (CHECK RESOURCES!!!)
# this will merge reads with a minimum overlap of 10 (-v)
# the minimum length of the merged reads is 350 (-n)
# for short insert sizes it might be recommented to set a maximum length for the merged reads (-m). 
# Freakishly long reads generally indicate an error...

# j: threads
# v: overlap
# n: min insert length
# m: max insert length (expect 420)
# o: output just needs basename, other stuff is added by PEAR
# no trimming (q) enabled as trimmomatic did the work here.

#creating directory for output of merging
mkdir Merged

qsub -t 1-${NSAMPLE} -v PEAR=${PEAR} ${PEAR_qsub}

#cleaning up directories
mkdir ./Merged/Merged_logs
mv ${PEAR_qsub}.* ./Merged/Merged_logs
mv ./Merged/*.log ./Merged/Merged_logs


###step 4: quality control with FASTQC

#create directory for fastqc output
mkdir FastQC

qsub -t 1-${NSAMPLE} -v FASTQC=${FASTQC} ${FASTQC_qsub}

#cleaning up directories
mkdir ./FastQC/FastQC_logs
mv ${FASTQC_qsub}.* ./FastQC/FastQC_logs

# Some of the quality scoring done by FASTQC results in inappropriate WARN/FAIL statuses:
# summary stats other than the mean are set to 0, probably due to a lack of sequence
# representation. Thus the following code will parse through the fastqc output re-assign PASS/WARN/FAIL statuses
# based on per base sequence quality...

for i in $(seq 1 ${NSAMPLE})
do

  # pull out the section of the fastqc output
  awk '/^>>Per base sequence quality/,/^>>END_MODULE/' ./FastQC/$i".assembled_fastqc"/fastqc_data.txt | grep "^[0-9]"> ./FastQC/$i".assembled_fastqc"/fastqc_PBSQ.txt

  # check if the number of bins with a median quality value (column 3, $3 in awk) not equal to zero is equal
  # to the number of bins with a median qual of at least 15 and a 10th percentile (column 6, $6 in awk) of at
  # least quality 5.
  # if that's true, check if the quality has a median qual of 25 and 10th perc of at least 10 - assign PASS
  # else WARN 
  # else FAIL
  # Based on these results, modify the per base seq qual (row 2) of the summary.txt file and capture
  # the results in a new summary file: summary1.txt.

  if [ $(awk '$3!=0' ./FastQC/$i".assembled_fastqc"/fastqc_PBSQ.txt | wc -l) -eq $(awk '$3>=15 && $6>=5' ./FastQC/$i".assembled_fastqc"/fastqc_PBSQ.txt| wc -l) ];
  then
    if [ $(awk '$3!=0' ./FastQC/$i".assembled_fastqc"/fastqc_PBSQ.txt | wc -l) -eq $(awk '$3>=25 && $6>=10' ./FastQC/$i".assembled_fastqc"/fastqc_PBSQ.txt | wc -l) ];
    then
      awk '{if(NR==2)$1="PASS\t"; print $0}' ./FastQC/$i".assembled_fastqc"/summary.txt > ./FastQC/$i".assembled_fastqc"/summary1.txt 
    else
      awk '{if(NR==2)$1="WARN\t"; print $0}' ./FastQC/$i".assembled_fastqc"/summary.txt > ./FastQC/$i".assembled_fastqc"/summary1.txt 
    fi
  else
    awk '{if(NR==2)$1="FAIL\t"; print $0}' ./FastQC/$i".assembled_fastqc"/summary.txt > ./FastQC/$i".assembled_fastqc"/summary1.txt 
  fi

done

# output some diagnostic files
#combine flags of 'Per base sequence quality' module for all files
grep "Per base sequence quality" ./FastQC/[0-9]*.assembled_fastqc/summary1.txt > ./FastQC/QC_summary.txt

#range of read lengths
grep "Sequence length" ./FastQC/[0-9]*.assembled_fastqc/fastqc_data.txt > ./FastQC/QC_read_length.txt

#combine flags of 'Sequence Length Distribution' module for all files including most abundant read lengths
for i in $(seq 1 ${NSAMPLE})
do
  awk '/^>>Sequence Length Distribution/,/^>>END_MODULE/' ./FastQC/$i".assembled_fastqc"/fastqc_data.txt |\
  sed -e '1,2d' -e '$d' > ./FastQC/$i".assembled_fastqc"/fastqc_SLD.txt

  sort -t$'\t' -k2nr ./FastQC/$i".assembled_fastqc"/fastqc_SLD.txt |\
  head -1 |\
  paste <(grep "Sequence Length Distribution" ./FastQC/$i".assembled_fastqc"/summary1.txt) - 
done > ./FastQC/QC_read_distribution.txt

#count sequences
#only counting forward read as representative for PE
grep -c '^@MISEQ' ./Renamed/[0-9]*_R1.fastq > nSeqs_all.txt
grep -c '^@MISEQ' ./Clipped/[0-9]*_clip_R1.fastq >> nSeqs_all.txt
grep -c '^@MISEQ' ./Trimmed/[0-9]*_ptrim_R1.fastq >> nSeqs_all.txt
grep -c '^@MISEQ' ./Merged/[0-9]*.assembled.fastq >> nSeqs_all.txt


###step 5: swarm OTU clustering (https://github.com/torognes/swarm)

#create directory for swarm input and output
mkdir Swarm

#extract fasta file from fastq and move to new directory
#requires at least jre1.8
# set fastawrap to 1000 to prevent line breaks within sequence

qsub -t 1-${NSAMPLE} -v REFORMAT=${REFORMAT} ${REFORMAT_qsub}

#cleaning up directory
mkdir ./Swarm/Swarm_logs
mv ${REFORMAT_qsub}.* ./Swarm/Swarm_logs

# Now to dereplicate and rename individual reads to save compute and mental anguish downstream...
# The dereplication code is courtesy of the Swarm developers and can be found here:
# https://github.com/torognes/swarm/wiki/Working-with-several-samples

qsub -t 1-${NSAMPLE} ${DEREPLICATE_qsub}
mv ${DEREPLICATE_qsub}.* ./Swarm/Swarm_logs

# study level dereplication 

cd ./Swarm/

export LC_ALL=C
cat *_dereplicated.fasta | \
awk 'BEGIN {RS = ">" ; FS = "[_\n]"}
     {if (NR != 1) {abundances[$1] += $2 ; sequences[$1] = $3}}
     END {for (amplicon in sequences) {
         print ">" amplicon "_" abundances[amplicon] "_" sequences[amplicon]}}' | \
sort --temporary-directory=$(pwd) -t "_" -k2,2nr -k1.2,1d | \
sed -e 's/\_/\n/2' > all_samples.fasta

#building amplicon contingency table (use script from swarm 1.20)
#the python script supplied with the latest versions of swarm may not work properly

python ../${AMPLICONTAB} *_dereplicated.fasta > amplicons_table.csv

#swarming
# -b light swarms have less than 3 reads associated with them
# -d 1: local edit distance threshold is 1 
# fastidious algorithm (-f): light swarms (amplicon abundance less than 3) will be grafted to heavy swarms
# -t set threads to 4
# -l output a log file
# -o the swarm file itself
# -s output a stats file (needed downstream)
# -w output fasta file with seed sequences

${SWARM} -b 3 -d 1 -f -t 4 -l ./Swarm_logs/swarm.log -o amplicons.swarms -s amplicons_stats.txt -w amplicons_seeds.fasta all_samples.fasta

# building OTU contingency table for multiple samples 
# https://github.com/torognes/swarm/wiki/Working-with-several-samples

# let the script know where the good stuff is...
STATS="amplicons_stats.txt"
SWARMS="amplicons.swarms"
AMPLICON_TABLE="amplicons_table.csv"
OTU_TABLE="OTU_contingency_table.csv"

# Header
echo -e "OTU\t$(head -n 1 "${AMPLICON_TABLE}")" > "${OTU_TABLE}"

# Compute "per sample abundance" for each OTU
awk -v SWARM="${SWARMS}" -v TABLE="${AMPLICON_TABLE}"  'BEGIN {FS = " "
            while ((getline < SWARM) > 0) {
                swarms[$1] = $0
            }
            FS = "\t"
            while ((getline < TABLE) > 0) {
                table[$1] = $0
            }
           }

     {# Parse the stat file (OTUs sorted by decreasing abundance)
      seed = $3 "_" $4
      n = split(swarms[seed], OTU, "[ _]")
      for (i = 1; i < n; i = i + 2) {
          s = split(table[OTU[i]], abundances, "\t")
          for (j = 1; j < s; j++) {
              samples[j] += abundances[j+1]
          }
      }
      printf "%s\t%s", NR, $3
      for (j = 1; j < s; j++) {
          printf "\t%s", samples[j]
      }
     printf "\n"
     delete samples
     }' "${STATS}" >> "${OTU_TABLE}"

# You may want to check if large swarms are taxonomically consistent
# by classifying more than their seed sequences.


###step 6: taxonomic classification

# At this stage, you could consider removing very rare swarms (less than one or two reads per swarm).
# As a large chunk of the swarms are rare (and will probably be removed from analysis later), you can save compute time here
# As always, whether this is advisable or not depends on your question.

#convert lowercase sequences to uppercase sequences in amplicons_seeds.fasta
awk '{print /^>/ ? $0 : toupper($0)}' amplicons_seeds.fasta > amplicons_seeds_uc.fasta
cd ..

# splitting seed sequence fasta file for parallel processing
mkdir Sina
cd Sina
asplit '^>' 500 < ../Swarm/amplicons_seeds_uc.fasta #split fasta file in fasta files with 500 sequences each

# Determine how many chunks there are...
JOBCOUNT=$(ls -1 out* | wc -l) #specify the number of files in array job

qsub -t 1-${JOBCOUNT} -v SINA=${SINA},SINA_PT=${SINA_PT} ../${SINA_qsub}

# Time to gather up the useful info from the split output... 
# In grep, -h suppresses printing of filenames for results

# Get all the swarm seed hashes (sort of like accessions)
grep -h '^sequence_identifier' $(ls -1v ${SINA_qsub}.o*) | sed 's/^sequence_identifier: //' > amplicons_seeds.accnos

#check if the order is the same as in amplicons_seeds_uc.fasta
grep '^>' amplicons_seeds_uc.fasta | sed 's/^>//' | diff - amplicons_seeds.accnos

# Get all corresponding taxonomic paths (note the same order as the accnos)
grep -h '^lca_tax_slv' $(ls -1v ${SINA_qsub}.o*) | sed 's/^lca_tax_slv: //' > amplicons_seeds.tax_slv

# Get all alignment qualities (for filtering later)
grep -h '^align_quality_slv' $(ls -1v ${SINA_qsub}.o*) | sed 's/^align_quality_slv: //' > amplicons_seeds.align_quality_slv

# merge these output files...
paste amplicons_seeds.accnos amplicons_seeds.align_quality_slv amplicons_seeds.tax_slv > amplicons_seeds_taxonomy.txt 
cd ..

#copy final output files to working directory
cp ./Swarm/OTU_contingency_table.csv ./
cp ./Sina/amplicons_seeds_taxonomy.txt ./

###step 7: further analysis (R)
# use e.g. ReadAmplicon.R, SubsampleNGS.R, PlotHillNGS.R

